# Classifica√ß√£o de Regi√µes Terrestres atrav√©s de Imagens de Sat√©lite e Deep Learning

## üìå 1. Introdu√ß√£o

O planeta Terra √© um mosaico complexo e vibrante de diferentes regi√µes, cada uma com suas pr√≥prias caracter√≠sticas √∫nicas. Compreender essas regi√µes e suas intera√ß√µes √© fundamental para uma variedade de aplica√ß√µes, desde o monitoramento ambiental at√© a planejamento urbano.

Neste projeto, exploramos a aplica√ß√£o de t√©cnicas de Deep Learning para a classifica√ß√£o de diferentes tipos de regi√µes terrestres usando imagens de sat√©lite. As imagens de sat√©lite oferecem uma vis√£o abrangente e objetiva da superf√≠cie da Terra, tornando-as uma ferramenta valiosa para este tipo de an√°lise.

Utilizamos uma arquitetura de Rede Neural Convolucional (CNN) para o nosso modelo de Deep Learning, que √© treinado com um conjunto diversificado de imagens de sat√©lite. O modelo √© ent√£o capaz de classificar novas imagens em v√°rias categorias, identificando caracter√≠sticas como florestas, corpos d‚Äô√°gua, √°reas urbanas e muito mais.

Este projeto n√£o s√≥ demonstra o poder do Deep Learning na an√°lise e interpreta√ß√£o de imagens de sat√©lite, mas tamb√©m serve como um exemplo pr√°tico da aplica√ß√£o dessas t√©cnicas em um contexto do mundo real. Esperamos que este trabalho possa inspirar outros a explorar ainda mais as possibilidades oferecidas pelo Deep Learning na an√°lise de imagens de sat√©lite.

## ‚öôÔ∏è 2. Configura√ß√µes de Ambiente 

Antes de come√ßarmos a construir e treinar nossa rede neural, precisamos configurar nosso ambiente. Isso envolve v√°rias etapas importantes que garantem que nosso c√≥digo seja executado corretamente e nossos experimentos sejam reproduz√≠veis.

### 2.1 Instala√ß√£o e Carga de Pacotes

- **os**: Esta √© uma biblioteca padr√£o do Python que fornece fun√ß√µes para interagir com o sistema operacional, incluindo a leitura de vari√°veis de ambiente, manipula√ß√£o de diret√≥rios e arquivos.
- **warnings**: Outra biblioteca padr√£o do Python usada para emitir avisos e controlar se eles s√£o ignorados ou n√£o.
- **numpy**: Uma biblioteca fundamental para a computa√ß√£o cient√≠fica em Python. Ela fornece suporte para arrays multidimensionais, matrizes e uma grande cole√ß√£o de fun√ß√µes matem√°ticas de alto n√≠vel.
- **seaborn** e **matplotlib**: S√£o bibliotecas de visualiza√ß√£o de dados em Python que fornecem uma interface de alto n√≠vel para desenhar gr√°ficos estat√≠sticos atraentes e informativos.
- **torchvision**: Uma parte do ecossistema PyTorch, torchvision √© usada para carregar e preparar conjuntos de dados de imagens, al√©m de fornecer alguns modelos pr√©-treinados.
- **torch**: √â uma biblioteca de aprendizado profundo de c√≥digo aberto que fornece uma interface flex√≠vel e eficiente para algoritmos de aprendizado profundo e usa a biblioteca de matrizes Tensor do Google.
- **torchmetrics**: √â uma biblioteca que fornece m√©tricas para avalia√ß√£o de modelos PyTorch, neste caso, a precis√£o da classifica√ß√£o.
- **lightning**: PyTorch Lightning √© uma estrutura leve que organiza o c√≥digo PyTorch, fornecendo abstra√ß√£o para treinamento, valida√ß√£o, teste e previs√£o enquanto mant√©m total controle e simplicidade.


## ü§ñ 3. Modelagem Com DenseNet121

<center><img src="https://i.ytimg.com/vi/wh-n-pTxMZU/maxresdefault.jpg" width=75%;></center>


### 3.1 Introdu√ß√£o √† Arquitetura DenseNet121

DenseNet, abrevia√ß√£o de Densely Connected Convolutional Networks, √© uma arquitetura de rede neural inovadora que utiliza conex√µes densas entre as camadas para aprimorar o desempenho das redes neurais convolucionais. Essa arquitetura avan√ßada tem demonstrado efic√°cia significativa em uma ampla gama de tarefas de vis√£o computacional, como classifica√ß√£o de imagens, detec√ß√£o de objetos e segmenta√ß√£o.

A arquitetura DenseNet121 √© uma variante espec√≠fica da DenseNet que consiste em v√°rias camadas convolucionais. Cada camada √© seguida por uma opera√ß√£o de concatena√ß√£o que combina a entrada de todas as camadas anteriores. Em resumo, a DenseNet121 √© composta pelas seguintes camadas:

- 1 convolu√ß√£o 7x7

- 58 convolu√ß√µes 3x3

- 61 convolu√ß√µes 1x1

- 4 AvgPool

- 1 camada totalmente conectada

Uma caracter√≠stica distintiva da DenseNet √© que cada camada est√° diretamente conectada a todas as outras camadas. Portanto, para 'L' camadas, existem L (L+1)/2 conex√µes diretas. Isso contrasta com as redes neurais convencionais, onde cada camada est√° conectada apenas √† pr√≥xima camada.

Outro componente importante da DenseNet s√£o os DenseBlocks. A opera√ß√£o de concatena√ß√£o n√£o √© vi√°vel quando o tamanho dos mapas de recursos muda. No entanto, uma parte essencial das CNNs √© a redu√ß√£o de dimensionalidade das camadas, que reduz o tamanho dos mapas de recursos para obter velocidades de computa√ß√£o mais altas. Para permitir isso, as DenseNets s√£o divididas em DenseBlocks, onde as dimens√µes dos mapas de recursos permanecem constantes dentro de um bloco, mas o n√∫mero de filtros entre eles √© alterado.

: Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).

### 3.2 Estrutura do DenseNet121

#### 3.2.1. Algoritmo de Otimiza√ß√£o Adamax

Adamax √© uma variante do algoritmo de otimiza√ß√£o Adam, que √© baseado em estimativas adaptativas de momentos de ordem inferior. Enquanto Adam usa a m√©dia m√≥vel exponencial do gradiente e do quadrado do gradiente para calcular as taxas de aprendizado adaptativas para cada par√¢metro, Adamax usa a norma infinita dos gradientes passados para o c√°lculo da taxa de aprendizado.

Aqui est√£o as equa√ß√µes matem√°ticas que definem o algoritmo Adamax:

Dado um gradiente $g_t$ no tempo $t$, o algoritmo Adamax atualiza os par√¢metros $\theta$ da seguinte maneira:

1. Atualiza a m√©dia m√≥vel exponencial do gradiente:

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$

2. Atualiza a norma infinita dos gradientes passados:

$$u_t = \max(\beta_2 u_{t-1}, |g_t|)$$

3. Atualiza os par√¢metros:

$$\theta_{t+1} = \theta_t - \frac{\eta}{u_t} m_t$$

##### Defini√ß√µes dos termos

- $\beta_1$: √â o hiperpar√¢metro que controla a taxa de decaimento da m√©dia m√≥vel exponencial do gradiente. Ele geralmente √© definido como um valor pr√≥ximo a 1 (por exemplo, 0.9).

- $\beta_2$: √â o hiperpar√¢metro que controla a taxa de decaimento da norma infinita dos gradientes passados. Ele tamb√©m √© geralmente definido como um valor pr√≥ximo a 1 (por exemplo, 0.999).

- $\eta$: √â a taxa de aprendizado. Este √© um hiperpar√¢metro que determina o tamanho do passo que o algoritmo d√° em cada itera√ß√£o.

- $m_t$: √â a m√©dia m√≥vel exponencial do gradiente no tempo $t$. Ela √© calculada como uma m√©dia ponderada do gradiente atual e da m√©dia m√≥vel exponencial anterior.

- $u_t$: √â a norma infinita dos gradientes passados at√© o tempo $t$. Ela √© calculada como o m√°ximo entre a norma infinita anterior e o valor absoluto do gradiente atual.

- $g_t$: √â o gradiente no tempo $t$. Ele √© calculado a partir da fun√ß√£o de perda.

#### 3.3.2. Fun√ß√£o de Perda Cross Entropy

A fun√ß√£o de perda de entropia cruzada √© uma fun√ß√£o de perda amplamente utilizada em aprendizado de m√°quina, especialmente em problemas de classifica√ß√£o. Ela √© usada para quantificar a diferen√ßa entre duas distribui√ß√µes de probabilidade. A ideia principal por tr√°s da entropia cruzada √© medir o n√≠vel de dissimilaridade entre a distribui√ß√£o de probabilidade prevista pelo modelo e a distribui√ß√£o de probabilidade verdadeira.

Aqui est√£o as equa√ß√µes matem√°ticas que definem a fun√ß√£o de perda de entropia cruzada:

Dado um vetor de r√≥tulos verdadeiros $y$ e um vetor de previs√µes $\hat{y}$, a fun√ß√£o de perda de entropia cruzada $L$ √© calculada da seguinte maneira:

$$L = -\sum_{i} y_i \log(\hat{y}_i)$$

##### Defini√ß√µes dos termos

- $y_i$: √â o i-√©simo elemento do vetor de r√≥tulos verdadeiros. Em um problema de classifica√ß√£o multiclasse, $y_i$ √© geralmente 1 para a classe correta e 0 para todas as outras classes.

- $\hat{y}_i$: √â o i-√©simo elemento do vetor de previs√µes. Ele representa a probabilidade prevista da i-√©sima classe.

- $\log(\hat{y}_i)$: √â o logaritmo natural da probabilidade prevista da i-√©sima classe. O logaritmo √© usado para penalizar as previs√µes erradas.

- $-\sum_{i} y_i \log(\hat{y}_i)$: √â a soma dos produtos dos r√≥tulos verdadeiros e dos logaritmos das previs√µes correspondentes. Esta soma √© negativa porque queremos minimizar a fun√ß√£o de perda.

## üçÄ 4. Avalia√ß√£o do Modelo

Nesta se√ß√£o, vamos explorar a performance do nosso modelo de Redes Neurais Convolucionais (CNN) utilizando a arquitetura DenseNet com diferentes fun√ß√µes de ativa√ß√£o. As fun√ß√µes de ativa√ß√£o desempenham um papel crucial na determina√ß√£o da efic√°cia de um modelo de aprendizado profundo, influenciando a velocidade de converg√™ncia durante o treinamento e a precis√£o do modelo em dados de teste.

Vamos avaliar o desempenho do nosso modelo em termos de m√©tricas padr√£o como acur√°cia, precis√£o, recall e F1-score. Al√©m disso, vamos analisar as curvas de aprendizado durante o treinamento e valida√ß√£o para entender como diferentes fun√ß√µes de ativa√ß√£o afetam o processo de aprendizado.

As fun√ß√µes de ativa√ß√£o que vamos considerar incluem ReLU, ELU, Sigmoid, Tanh e LeakyReLU. Cada uma dessas fun√ß√µes tem suas pr√≥prias caracter√≠sticas e pode influenciar o desempenho do modelo de maneiras diferentes.

### 4.1 Fun√ß√µes de Ativa√ß√£o

As fun√ß√µes de ativa√ß√£o s√£o uma parte crucial das Redes Neurais, pois ajudam a introduzir a n√£o-linearidade no modelo. Aqui est√£o as descri√ß√µes r√°pidas e as f√≥rmulas matem√°ticas das fun√ß√µes de ativa√ß√£o que voc√™ mencionou:

1. **ReLU (Rectified Linear Unit)**: √â a fun√ß√£o de ativa√ß√£o mais comumente usada em redes neurais e deep learning. A fun√ß√£o retorna 0 se o input for negativo, e o pr√≥prio input se for positivo.
    * F√≥rmula: $$f(x) = max(0, x)$$

2. **ELU (Exponential Linear Unit)**: Semelhante √† ReLU, mas suaviza a fun√ß√£o para x < 0, o que pode acelerar a aprendizagem.
    * F√≥rmula: $$f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha(e^x - 1) & \text{if } x \leq 0 \end{cases}$$ onde $\alpha$ √© um par√¢metro positivo.

3. **Sigmoid**: √â uma fun√ß√£o que mapeia qualquer valor para um valor entre 0 e 1. √â √∫til para modelos onde precisamos prever a probabilidade como uma sa√≠da.
    * F√≥rmula: $$f(x) = \frac{1}{1 + e^{-x}}$$

4. **Tanh (Hyperbolic Tangent)**: √â semelhante √† fun√ß√£o sigmoid, mas mapeia os valores de entrada para um intervalo entre -1 e 1.
    * F√≥rmula: $$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

5. **LeakyReLU**: √â uma variante da ReLU que resolve o problema dos "neur√¥nios mortos" permitindo pequenos valores negativos quando o input √© menor que zero.
    * F√≥rmula: $$f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}$$ onde $\alpha$ √© um pequeno valor constante.

Cada uma dessas fun√ß√µes de ativa√ß√£o tem suas pr√≥prias vantagens e desvantagens, e a escolha da fun√ß√£o de ativa√ß√£o pode ter um grande impacto no desempenho do modelo de aprendizado profundo.

<img src="densenet121.png">

### 4.2 An√°lise dos Modelos em Dados de Teste

Nesta se√ß√£o, vamos analisar o desempenho dos nossos modelos DenseNet121 com diferentes fun√ß√µes de ativa√ß√£o nos dados de teste. Esta an√°lise √© crucial para entender como os modelos treinados generalizam para dados n√£o vistos anteriormente. A acur√°cia nos dados de teste √© uma m√©trica importante, pois fornece uma estimativa do desempenho do modelo em condi√ß√µes reais.

A tabela a seguir mostra a acur√°cia do DenseNet121 usando diferentes fun√ß√µes de ativa√ß√£o nos dados de teste:

| Fun√ß√£o de Ativa√ß√£o | Acur√°cia (%) |
|--------------------|--------------|
| LeakyReLU          | 99,64        |
| ReLU               | 99,05        |
| Tanh               | 86,86        |
| ELU                | 71,95        |
| Sigmoid            | 52,54        |

Estes resultados destacam a import√¢ncia da escolha da fun√ß√£o de ativa√ß√£o na performance do modelo. Na pr√≥xima se√ß√£o, vamos discutir mais detalhadamente esses resultados e suas implica√ß√µes.

##  üóΩ 5. Conclus√£o e Discuss√£o

As fun√ß√µes de ativa√ß√£o desempenham um papel crucial nas Redes Neurais Convolucionais (CNNs). Elas introduzem a n√£o-linearidade que torna as CNNs poderosas. Sem fun√ß√µes de ativa√ß√£o, n√£o importa quantas camadas uma rede neural tenha, ela se comportaria da mesma forma que um modelo linear √∫nico. As fun√ß√µes de ativa√ß√£o ajudam a rede neural a aprender a partir dos erros cometidos, ajustando os pesos durante o processo de retropropaga√ß√£o.

Neste projeto, exploramos o impacto de diferentes fun√ß√µes de ativa√ß√£o - ReLU, ELU, Sigmoid, Tanh e LeakyReLU - no desempenho do modelo DenseNet121. Os resultados mostraram varia√ß√µes significativas na acur√°cia do modelo com diferentes fun√ß√µes de ativa√ß√£o.

- **ReLU**: A ReLU teve um desempenho excepcionalmente bom, com uma acur√°cia de 99,05%. Isso n√£o √© surpreendente, pois a ReLU √© conhecida por sua efic√°cia em muitos tipos de redes neurais, principalmente por causa de sua simplicidade e capacidade de mitigar o problema do desaparecimento do gradiente.

- **ELU**: A ELU teve um desempenho inferior em compara√ß√£o com a ReLU, com uma acur√°cia de 71,95%. Embora a ELU possa ajudar a acelerar a converg√™ncia do aprendizado e produzir uma representa√ß√£o mais robusta, parece que n√£o foi t√£o eficaz quanto a ReLU neste caso.

- **Sigmoid**: A Sigmoid teve o desempenho mais baixo entre todas as fun√ß√µes de ativa√ß√£o testadas, com uma acur√°cia de 52,54%. Isso pode ser atribu√≠do ao fato de que a Sigmoid sofre do problema do desaparecimento do gradiente, o que pode dificultar o aprendizado do modelo.

- **Tanh**: A Tanh teve um desempenho melhor do que a Sigmoid, mas ainda assim inferior √† ReLU e √† LeakyReLU, com uma acur√°cia de 86,86%. Embora a Tanh seja semelhante √† Sigmoid, ela mapeia os valores para um intervalo entre -1 e 1, o que pode ter contribu√≠do para seu melhor desempenho em compara√ß√£o com a Sigmoid.

- **LeakyReLU**: A LeakyReLU teve o melhor desempenho entre todas as fun√ß√µes de ativa√ß√£o testadas, com uma acur√°cia de 99,64%. Isso sugere que permitir pequenos valores negativos quando o input √© menor que zero pode melhorar o desempenho do modelo.

Estes resultados destacam a import√¢ncia da escolha da fun√ß√£o de ativa√ß√£o no desempenho dos modelos CNN. No entanto, √© importante notar que esses resultados s√£o espec√≠ficos para este conjunto de dados e para o modelo DenseNet121. Outros modelos ou conjuntos de dados podem produzir resultados diferentes. Portanto, √© sempre uma boa pr√°tica experimentar diferentes fun√ß√µes de ativa√ß√£o ao treinar modelos CNN.
